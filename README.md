# PaLI-A-Jointly-Scaled-Multilingual-Language-Image-Model-Paper-Presentation

# Outline
- [Overview](#Overview)
- [Model Architecture](#Model Architecture)
- [Data](#Data)
- [Pretraining Tasks](#Pretraining Tasks)
- [Testing](#Testing)
- [Limitations/Biases](#Limitations/Biases)
- [Critical Analysis](#Critical Analysis)
- [Link](#Link)

# Overview

> Increasing neural network capacity has been a successful trend in the modeling of language and vision tasks. Language models such as T5 and GPT-3 have shown significant advantages from training large Transformers on large amounts text data. On the other hand, vision models such as CNNs and Vision Transformers have seen similar benefits from scaling but to a lesser extent compared to language models. Language-and-vision modeling are also popular now dealing with problems like Image Captioning and Visual Question-Answering. 

# Model Architecture

# Data


# Pretraining Tasks


# Testing


# Limitations/Biases


# Critical Analysis


# Link
